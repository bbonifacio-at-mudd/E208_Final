{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f56394",
   "metadata": {},
   "source": [
    "# In this cell, we will perform Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35589c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17453d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we define our data loader. \n",
    "#Much of this is from HW6\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "def prepare_and_get_loaders(validation_split, batch_size, num_workers, dir_path):\n",
    "    \"\"\"\n",
    "    This function loads and partitions our image data.\n",
    "    \n",
    "    val_split: proportion of the data that will be in the validation split\n",
    "    batch_size: size of batches\n",
    "    num_workers: number of parallel processors (-1 to use all)\n",
    "    dir_path: directory path of the data\n",
    "    \"\"\"\n",
    "    \n",
    "    #Transforms to apply to training and test data\n",
    "    #Useful augmentation: transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "        transforms.Resize((244, 244)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) \n",
    "    ]),\n",
    "\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((244,244)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "    }\n",
    "    full_dataset = datasets.ImageFolder(dir_path)\n",
    "    \n",
    "    m = len(full_dataset)\n",
    "    val_size = int(validation_split * m)\n",
    "    train_size = m - val_size\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    train_dataset = MyDataset(train_dataset, transform=data_transforms['train'])\n",
    "    val_dataset = MyDataset(val_dataset, transform=data_transforms['test'])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c13c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, train_loader, val_loader, learning_rate, num_epochs, L2, momentum1, momentum2, stepsize, gamma):\n",
    "    \"\"\"\n",
    "\n",
    "    Finetunes the given model on the loader data, and returns the trained model as well as the losses\n",
    "    and final validation accuracy for plotting. \n",
    "    \n",
    "    model: the Pytorch model\n",
    "    train_loader: the loader for the training data\n",
    "    val_loader: the loader for the validation data\n",
    "    learning_rate: alpha, the learning rate\n",
    "    num_epochs: the number of epochs to train for\n",
    "    L2: the L2 regularization value\n",
    "    momentum1: the first momentum value\n",
    "    momentum2: the second momentum value\n",
    "    stepsize: the number of epochs between lowering the learning rate\n",
    "    gamma: the ratio to multiply the learning rate at each step size\n",
    "\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"My Device is {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2, betas = (momentum1, momentum2))\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=stepsize, gamma=gamma)\n",
    "\n",
    "    hist = {'train': [], 'val': []} # History of training and validation losses\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc = \"Training Epoch: \"):\n",
    "        model.train() #Training mode\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            #put stuff on device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #zero optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #get outputs + train\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #get loss and do backward\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            #step\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        hist['train'].append(avg_train_loss)\n",
    "\n",
    "        #Validation Mode Now\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        #keep track of correct labels so we can get the val accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                #put on device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                #get outputs\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                #get loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                #https://www.digitalocean.com/community/tutorials/pytorch-torch-max\n",
    "                max_element, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = total_loss / len(val_loader)\n",
    "        hist['val'].append(avg_val_loss)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    #final val accuracy\n",
    "    final_val_accuracy = 100*correct/total\n",
    "\n",
    "    return model, hist, final_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c9dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(arch, dropout, num_classes):\n",
    "    \"\"\"\n",
    "\n",
    "    Loads a pretrained model, but with the final output layer removed and replaced with a randomly\n",
    "    initialized output layer that will classify among the 5 buildings.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #select the model we want\n",
    "    if arch == \"resnet18\": \n",
    "        model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Sequential(nn.Dropout(dropout), nn.Linear(num_ftrs, num_classes)) \n",
    "    elif arch == \"mobile\":\n",
    "        model = models.mobilenet_v2(weights=\"MobileNet_V2_Weights.DEFAULT\")\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(nn.Dropout(dropout), nn.Linear(num_ftrs, num_classes))\n",
    "    else:\n",
    "        print(\"Incorrect model Architecture selected\")\n",
    "        raise\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset parameters\n",
    "validation_split = 0.2\n",
    "batch_size = 16\n",
    "num_workers = -1\n",
    "dir_path = 'Data_400'\n",
    "\n",
    "train_loader, val_loader = prepare_and_get_loaders(validation_split, batch_size, num_workers, dir_path):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e652bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to C:\\Users\\Brandon/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-7ebf99e0.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbb9ef1cc93489c90dd26b22cd87183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Model Parameters\n",
    "arch = \"mobile\"\n",
    "dropout = 0.3\n",
    "num_classes = 5\n",
    "model = load_model(arch, dropout, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finetuning Parameters\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "L2 = 10**(-5)\n",
    "momentum1 = 0.9\n",
    "momentum2 = 0.99\n",
    "stepsize = 5\n",
    "gamma = 0.1\n",
    "model, hist, final_val_accuracy = finetune(model,\n",
    "                                          train_loader,\n",
    "                                          val_loader,\n",
    "                                          learning_rate,\n",
    "                                          num_epochs,\n",
    "                                          L2,\n",
    "                                          momentum1,\n",
    "                                          momentum2, \n",
    "                                          stepsize,\n",
    "                                          gamma)\n",
    "print(f\"Our final validation accuracy accuracy using the full CNN is {final_val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist):\n",
    "    epochs = range(1, len(hist['train']) + 1)\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(epochs, hist['train'], 'b', label='Training loss')\n",
    "    plt.plot(epochs, hist['val'], 'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8b8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d108442a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
